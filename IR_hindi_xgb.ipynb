{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V89sx17EOVrD"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "!pip install transformers\n",
    "import transformers\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "\n",
    "\n",
    "def multi_hot_encoder(labels):\n",
    "  label_array = [label.replace(\" \", \"\").split(',') for label in labels]\n",
    "  label_dict = {'offensive': 0, 'fake': 1, 'defamation': 2, 'hate': 3, 'non-hostile': 4}\n",
    "  labels = np.zeros(shape=(len(label_array), len(label_dict)))\n",
    "  for i, label in enumerate(label_array):\n",
    "    for l in label:\n",
    "      labels[i][(label_dict[l])] = 1\n",
    "  \n",
    "  return labels\n",
    "\n",
    "\n",
    "def binary_encoder(labels):\n",
    "  label_array = [label.split(',') for label in labels]\n",
    "  label_dict = {'offensive': 0, 'fake': 1, 'defamation': 2, 'hate': 3, 'non-hostile': 4}\n",
    "  labels = np.zeros(shape=len(label_array), dtype=np.int32)\n",
    "  for i, label in enumerate(label_array):\n",
    "    if label_dict[label[0]] < 4:\n",
    "      labels[i] = 1\n",
    "\n",
    "  return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "1db6a8a47f954675982e1dbd461b1bb4",
      "70b49c89b2ff486fb5e03ab4696553cb",
      "ca0452d8653e4564973b64a040ab96e4",
      "b7a430d659654017b3b61d0d2c478bc9",
      "34995af025c94607a27b5706b30b159b",
      "400e162c8cf1437b98ac8f9077471b39",
      "9e7665eeab174596b2e18e46d2c1f4ee",
      "05bc28ede34640a68ced2a518d5aabf9"
     ]
    },
    "id": "9QoswPqwjy2r",
    "outputId": "06120753-6394-4af4-9f0a-8ac73d24be25"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db6a8a47f954675982e1dbd461b1bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=871891.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\"\n",
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "BERT_PATH = \"bert-base-multilingual-uncased\"\n",
    "MODEL_PATH = \"/content/drive//My Drive/IR_Hindi/Models_Multi/model7.bin\"\n",
    "MODEL_PATH_BINARY = \"/content/drive//My Drive/IR_Hindi/Models_Binary/model3.bin\"\n",
    "TRAINING_FILE = \"/content/drive//My Drive/IR_Hindi/train.csv\"\n",
    "VALIDATION_FILE = \"/content/drive//My Drive/IR_Hindi/Constraint_Hindi_Valid - Sheet1.csv\"\n",
    "TEST_FILE = \"/content/drive//My Drive/IR_Hindi/Test Set - test.csv\"\n",
    "FILE_SAVE_PATH = \"/content/drive//My Drive/IR_Hindi/Techaos_Valid_7.csv\"\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciVIN9XnYctS"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, review, target):\n",
    "        self.review = review\n",
    "        self.target = target\n",
    "        self.tokenizer = TOKENIZER\n",
    "        self.max_len = MAX_LEN\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.review)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.review[item])\n",
    "        review = \" \".join(review.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"targets\": torch.tensor(self.target[item], dtype=torch.float),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TJfescALgC2h"
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n",
    "        self.bert_drop = nn.Dropout(0.2)\n",
    "        self.lin1 = nn.Linear(768, 256)\n",
    "        self.lin2 = nn.Linear(256, 5)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        o = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        bo = self.bert_drop(o.pooler_output)\n",
    "        # output = nn.functional.relu(self.lin1(bo))\n",
    "        # output = self.lin2(output)\n",
    "        return bo\n",
    "\n",
    "class BERT_Binary(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT_Binary, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n",
    "        self.bert_drop = nn.Dropout(0.2)\n",
    "        self.lin1 = nn.Linear(768, 256)\n",
    "        self.lin2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        o = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        bo = self.bert_drop(o.pooler_output)\n",
    "        output = nn.functional.relu(self.lin1(bo))\n",
    "        output = self.lin2(output)\n",
    "        return output\n",
    "\n",
    "def sentence_prediction(MODEL, sentence):\n",
    "    tokenizer = TOKENIZER\n",
    "    max_len = MAX_LEN\n",
    "    review = str(sentence)\n",
    "    review = \" \".join(review.split())\n",
    "\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        review, None, add_special_tokens=True, max_length=max_len, truncation=True,\n",
    "    )\n",
    "\n",
    "    ids = inputs[\"input_ids\"]\n",
    "    mask = inputs[\"attention_mask\"]\n",
    "    token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "    padding_length = max_len - len(ids)\n",
    "    ids = ids + ([0] * padding_length)\n",
    "    mask = mask + ([0] * padding_length)\n",
    "    token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "\n",
    "    ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n",
    "    mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)\n",
    "    token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    ids = ids.to(DEVICE, dtype=torch.long)\n",
    "    token_type_ids = token_type_ids.to(DEVICE, dtype=torch.long)\n",
    "    mask = mask.to(DEVICE, dtype=torch.long)\n",
    "\n",
    "    outputs = MODEL(ids=ids, mask=mask, token_type_ids=token_type_ids)\n",
    "\n",
    "    outputs = torch.sigmoid(outputs).cpu().detach().numpy()\n",
    "    return outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P02zbSemW8qV"
   },
   "outputs": [],
   "source": [
    "bert = BERT()\n",
    "bert.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device(DEVICE)))\n",
    "df_train = pd.read_csv(TRAINING_FILE)\n",
    "df_valid = pd.read_csv(VALIDATION_FILE)\n",
    "binaryClassification = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert.to(device)\n",
    "bert.eval()\n",
    "for i,v in enumerate(df_train['Post']):\n",
    "  if type(v)==float:\n",
    "    break\n",
    "  v = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', ' ', v, flags=re.MULTILINE)\n",
    "  df_train.loc[i,'Post'] = v\n",
    "\n",
    "for i,v in enumerate(df_valid['Post']):\n",
    "  if type(v)==float:\n",
    "    break\n",
    "  v = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', ' ', v, flags=re.MULTILINE)\n",
    "  df_valid.loc[i,'Post'] = v\n",
    "\n",
    "if binaryClassification:\n",
    "  train_targets = binary_encoder(df_train['Labels Set'])\n",
    "  valid_targets = binary_encoder(df_valid['Labels Set'])\n",
    "else:\n",
    "  train_targets = multi_hot_encoder(df_train['Labels Set'])\n",
    "  valid_targets = multi_hot_encoder(df_valid['Labels Set'])\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_valid = df_valid.reset_index(drop=True)\n",
    "\n",
    "train_dataset = Dataset(review=df_train['Post'], target=train_targets)\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=4)\n",
    "\n",
    "valid_dataset = Dataset(review=df_valid['Post'], target=valid_targets)\n",
    "\n",
    "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YLq64doXoEX"
   },
   "outputs": [],
   "source": [
    "def getSentenceEmbeddings(data_loader, model, device):\n",
    "  model.eval()\n",
    "  fin_targets = []\n",
    "  fin_outputs = []\n",
    "  with torch.no_grad():\n",
    "    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "      ids = d[\"ids\"]\n",
    "      token_type_ids = d[\"token_type_ids\"]\n",
    "      mask = d[\"mask\"]\n",
    "      targets = d[\"targets\"]\n",
    "\n",
    "      ids = ids.to(device, dtype=torch.long)\n",
    "      token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "      mask = mask.to(device, dtype=torch.long)\n",
    "      targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "      outputs = torch.sigmoid(model(ids, mask=mask, token_type_ids=token_type_ids))\n",
    "      fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "      fin_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
    "  return np.array(fin_outputs), np.array(fin_targets)\n",
    "\n",
    "def eval_fn(data_loader, model, device):\n",
    "  model.eval()\n",
    "  fin_targets = []\n",
    "  fin_outputs = []\n",
    "  with torch.no_grad():\n",
    "    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "      ids = d[\"ids\"]\n",
    "      token_type_ids = d[\"token_type_ids\"]\n",
    "      mask = d[\"mask\"]\n",
    "      targets = d[\"targets\"]\n",
    "\n",
    "      ids = ids.to(device, dtype=torch.long)\n",
    "      token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "      mask = mask.to(device, dtype=torch.long)\n",
    "      targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "      outputs = torch.sigmoid(model(ids=ids, mask=mask, token_type_ids=token_type_ids))\n",
    "      fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "      fin_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
    "  return fin_outputs, fin_targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNfTttyKZZdW",
    "outputId": "224ecaa8-a175-402b-c7a6-66d1939a2be0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 716/716 [00:49<00:00, 14.42it/s]\n",
      "100%|██████████| 203/203 [00:07<00:00, 26.87it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, train_targets = getSentenceEmbeddings(train_data_loader, bert, device)\n",
    "valid_data, valid_targets = getSentenceEmbeddings(valid_data_loader, bert, device)\n",
    "\n",
    "# inds = []\n",
    "# for i, target in enumerate(train_targets):\n",
    "#   if target[4] == 0:\n",
    "#     inds.append(i)\n",
    "\n",
    "# train_data = train_data[inds]\n",
    "# train_targets = (train_targets[inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sbZEP48ncchU",
    "outputId": "89e6d9c5-099a-4a56-880c-a5077426896f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=-1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "offensive = XGBClassifier(n_jobs=-1, objective=\"binary:logistic\", max_depth=4)\n",
    "fake = XGBClassifier(n_jobs=-1, objective=\"binary:logistic\", max_depth=4)\n",
    "defamation = XGBClassifier(n_jobs=-1, objective=\"binary:logistic\", max_depth=4)\n",
    "hate = XGBClassifier(n_jobs=-1, objective=\"binary:logistic\", max_depth=4)\n",
    "non_hostile = XGBClassifier(n_jobs=-1, objective=\"binary:logistic\", max_depth=4)\n",
    "\n",
    "offensive.fit(train_data, train_targets[:,0])\n",
    "fake.fit(train_data, train_targets[:,1])\n",
    "defamation.fit(train_data, train_targets[:,2])\n",
    "hate.fit(train_data, train_targets[:,3])\n",
    "non_hostile.fit(train_data, train_targets[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVa3cYGbrAn1"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "binaryModel = BERT_Binary()\n",
    "binaryModel.load_state_dict(torch.load(MODEL_PATH_BINARY, map_location=torch.device(device)))\n",
    "binaryModel.to(device)\n",
    "\n",
    "binary_preds, _ = eval_fn(valid_data_loader, binaryModel, device)\n",
    "# binary_preds = []\n",
    "# for post in df_valid['Post']:\n",
    "#   binary_preds.append(sentence_prediction(binaryModel, post)[0])\n",
    "binary_targets = binary_encoder(df_valid['Labels Set'])\n",
    "\n",
    "binary_preds = np.array(binary_preds) >= 0.5\n",
    "preds = np.zeros((binary_preds.shape[0] ,5))\n",
    "preds[:,0] = offensive.predict(valid_data)\n",
    "preds[:,1] = fake.predict(valid_data)\n",
    "preds[:,2] = defamation.predict(valid_data)\n",
    "preds[:,3] = hate.predict(valid_data)\n",
    "preds[:,4] = non_hostile.predict(valid_data)\n",
    "\n",
    "for i in range(preds.shape[0]):\n",
    "  if binary_preds[i] == 0:\n",
    "    preds[i] = np.zeros(5)\n",
    "\n",
    "f1_score = metrics.f1_score(valid_targets[:,:4], preds[:, :4], average=None)\n",
    "print(\"Classwise F1: \",f1_score)\n",
    "f1_score = metrics.f1_score(valid_targets[:,:4], preds[:, :4], average=\"weighted\")\n",
    "print(\"Fine grained F1: \", f1_score)\n",
    "f1 = metrics.f1_score(binary_targets, binary_preds, average=\"weighted\")\n",
    "print(\"Binary Classification: \", f1)\n",
    "dataAnalyzer(valid_targets[:,:4], preds[:,:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7zqCDzN1on6o",
    "outputId": "7fcb1e9b-1566-448f-f6a5-f4a976ece379"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 203/203 [00:13<00:00, 15.06it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(TEST_FILE)\n",
    "dummy_targets = np.zeros((len(df_test['Post']), 5))\n",
    "test_dataset = Dataset(review=df_test['Post'], target=dummy_targets)\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=VALID_BATCH_SIZE, num_workers=1)\n",
    "test_data, _ = getSentenceEmbeddings(test_data_loader, bert, device)\n",
    "\n",
    "binary_preds = []\n",
    "for post in df_test['Post']:\n",
    "  binary_preds.append(sentence_prediction(binaryModel, post)[0])\n",
    "binary_preds = np.array(binary_preds) >= 0.5\n",
    "\n",
    "preds = np.zeros((binary_preds.shape[0] ,4))\n",
    "preds[:,0] = offensive.predict(test_data)\n",
    "preds[:,1] = fake.predict(test_data)\n",
    "preds[:,2] = defamation.predict(test_data)\n",
    "preds[:,3] = hate.predict(test_data)\n",
    "\n",
    "for i in range(preds.shape[0]):\n",
    "  if binary_preds[i] == 0:\n",
    "    preds[i] = np.zeros(4)\n",
    "\n",
    "outs = []\n",
    "label_dict = {0: 'offensive', 1: 'fake', 2: 'defamation', 3: 'hate', 4: 'non-hostile'}\n",
    "for pred, binary_pred in zip(preds, binary_preds):\n",
    "  s = \"\"\n",
    "  if binary_pred == 0:\n",
    "    s = \"non-hostile, \"\n",
    "  else:\n",
    "    for j, val in enumerate(pred):\n",
    "      if val == 1:\n",
    "        s += (label_dict[j] + ', ')\n",
    "  outs.append(s[:-2])\n",
    "\n",
    "to_save = list(zip(range(1,len(outs)+1), outs))\n",
    "df = pd.DataFrame(to_save, \n",
    "               columns =['Unique ID', 'Labels Set'])\n",
    "df.to_csv(FILE_SAVE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OJEs-TKrj2Y"
   },
   "outputs": [],
   "source": [
    "def dataAnalyzer(targets, outputs):\n",
    "  label_dict = {'offensive': 0, 'fake': 1, 'defamation': 2, 'hate': 3}\n",
    "  predCounts = [0, 0, 0, 0]\n",
    "  realCounts = [0, 0, 0, 0]\n",
    "  inpredCounts = [0, 0, 0, 0]\n",
    "  inrealCounts = [0, 0, 0, 0]\n",
    "\n",
    "  for i in range(len(targets)):\n",
    "    for j in range(len(label_dict)):\n",
    "      if targets[i][j] == 1:\n",
    "        realCounts[j] += 1\n",
    "        if targets[i][j] == outputs[i][j]:\n",
    "          predCounts[j] += 1\n",
    "      else:\n",
    "        inrealCounts[j] += 1\n",
    "        if targets[i][j] == outputs[i][j]:\n",
    "          inpredCounts[j] += 1\n",
    "\n",
    "  print(\"True +ve\")\n",
    "  print(predCounts)\n",
    "  print(\"Total +ve\")\n",
    "  print(realCounts)\n",
    "  print(\"True -ve\")\n",
    "  print(inpredCounts)\n",
    "  print(\"Total -ve\")\n",
    "  print(inrealCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkxfVGPdyIT-"
   },
   "outputs": [],
   "source": [
    "# The final weighted average of the four submissions to create the fifth submission.\n",
    "\n",
    "dir_path = \"/content/drive//My Drive/IR_Hindi/\"\n",
    "df_1 = pd.read_csv(dir_path + \"Quark_1.csv\")\n",
    "df_2 = pd.read_csv(dir_path + \"Quark_2.csv\")\n",
    "df_3 = pd.read_csv(dir_path + \"Quark_3.csv\")\n",
    "df_4 = pd.read_csv(dir_path + \"Quark_4.csv\")\n",
    "\n",
    "weights = np.array([0.58, 0.6088, 0.5832, 0.6149])\n",
    "total_weight = weights.sum()\n",
    "\n",
    "preds_1 = multi_hot_encoder(df_1['Labels Set'])\n",
    "preds_2 = multi_hot_encoder(df_2['Labels Set'])\n",
    "preds_3 = multi_hot_encoder(df_3['Labels Set'])\n",
    "preds_4 = multi_hot_encoder(df_4['Labels Set'])\n",
    "\n",
    "preds_5 = ((preds_1*weights[0] + preds_2*weights[1] + preds_3*weights[2] + preds_4*weights[3])/total_weight) >= 0.5\n",
    "\n",
    "outs = []\n",
    "label_dict = {0: 'offensive', 1: 'fake', 2: 'defamation', 3: 'hate', 4: 'non-hostile'}\n",
    "for pred in preds_5:\n",
    "  s = \"\"\n",
    "  for j, val in enumerate(pred):\n",
    "    if val == 1:\n",
    "      s += (label_dict[j] + ', ')\n",
    "  outs.append(s[:-2])\n",
    "\n",
    "to_save = list(zip(range(1,len(outs)+1), outs))\n",
    "df = pd.DataFrame(to_save, \n",
    "               columns =['Unique ID', 'Labels Set'])\n",
    "df.to_csv(dir_path + \"Quark_5.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IR-hindi_xgb.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05bc28ede34640a68ced2a518d5aabf9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1db6a8a47f954675982e1dbd461b1bb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ca0452d8653e4564973b64a040ab96e4",
       "IPY_MODEL_b7a430d659654017b3b61d0d2c478bc9"
      ],
      "layout": "IPY_MODEL_70b49c89b2ff486fb5e03ab4696553cb"
     }
    },
    "34995af025c94607a27b5706b30b159b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "400e162c8cf1437b98ac8f9077471b39": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70b49c89b2ff486fb5e03ab4696553cb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e7665eeab174596b2e18e46d2c1f4ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b7a430d659654017b3b61d0d2c478bc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05bc28ede34640a68ced2a518d5aabf9",
      "placeholder": "​",
      "style": "IPY_MODEL_9e7665eeab174596b2e18e46d2c1f4ee",
      "value": " 872k/872k [00:00&lt;00:00, 1.82MB/s]"
     }
    },
    "ca0452d8653e4564973b64a040ab96e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_400e162c8cf1437b98ac8f9077471b39",
      "max": 871891,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_34995af025c94607a27b5706b30b159b",
      "value": 871891
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
